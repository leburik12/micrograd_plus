## ğŸ” Overview

The core design remains faithful to Karpathyâ€™s minimalist implementation â€” 
a pure Python autodiff engine that supports backpropagation over scalar-valued computational graphs. 
This fork, however, **extends the engine with advanced neural network activation functions**, such as:

- âœ… `Softplus` (smooth ReLU alternative)
- âœ… `Swish` (sigmoid-weighted input)
- âœ… `LeakyReLU`, `ELU`, and more *(to be added incrementally)*

These additions make `micrograd_plus` suitable for **research-grade introspection**, 
**experimental optimization**, and **educational demonstrations** of gradient flow under various nonlinearities.

## ğŸ“ Directory Structure

```text

â”œâ”€â”€ micrograd_plus/
â”‚   â”œâ”€â”€ __init__.py       # Package initializer
â”‚   â”œâ”€â”€ engine.py         # Core autodiff engine (extended from micrograd)
â”‚   â”œâ”€â”€ nn.py             # MLP framework with support for custom activations

ğŸ“œ License

MIT â€” feel free to use and extend.

email:
leburikplc@gmail.com
